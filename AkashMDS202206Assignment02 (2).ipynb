{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNqp1HMVC84H81B96XC7U5w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bjro7xE3TFkE"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## Assignment 2 : Probabilistic Language Model"],"metadata":{"id":"0sRg39whTo0H"}},{"cell_type":"markdown","source":["# Part 1 (15 Marks)\n","### (1) Display the sentence count\n","### (2) Build a 4-gram language model.\n","### (3) Next word prediction - Test it to find the next word using the probability distribution.\n","â€¢ Take any word sequence (trigrams) and find the next word. Display the result of next\n","word prediction using 5 test sequences\n","### (4) Sentence generation - Using a random start word to construct a sentence of 10 words.\n","Generate 5 sentences and publish the result. Avoid using the, in, a or an as the starting\n","word"],"metadata":{"id":"Tt-RlJO6UMSJ"}},{"cell_type":"markdown","source":["#### Install and Import Required Libraries"],"metadata":{"id":"WhAaKLDlUtOW"}},{"cell_type":"code","source":["import nltk\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from bs4 import BeautifulSoup\n","from nltk import FreqDist\n","import matplotlib.pyplot as plt\n","# import mplcursors\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import regex as re\n","import pickle\n","import string\n","from collections import Counter\n","import nltk\n","from nltk import ngrams, FreqDist\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import random\n","import pickle\n","import nltk\n","from nltk import pos_tag\n","\n","\n","\n"],"metadata":{"id":"PPY-V6VaGEFF","executionInfo":{"status":"ok","timestamp":1697338961074,"user_tz":-330,"elapsed":2466,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cd197c16-46f7-4ff7-edc5-bde14f7a48c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"markdown","source":["#### Mount Google Drive to Load Data"],"metadata":{"id":"hb4vQcwcVX7f"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVBhhOvXGUFF","executionInfo":{"status":"ok","timestamp":1697338987373,"user_tz":-330,"elapsed":22660,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"ce09b8cd-e113-46ba-bf8e-50436404d767"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#### Store All Text Files Inside a List"],"metadata":{"id":"L9wTCRyHWbYk"}},{"cell_type":"code","source":["corpus_files = glob.glob(os.path.join(r\"/content/drive/MyDrive/NLP/Eng_Data/wiki\", '*.txt'))"],"metadata":{"id":"B4CTtW2eMCEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"QjYHytxRKK99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#forming final preprocessed corpus\n","# direc = os.listdir('drive/MyDrive/NLP')\n","final_corpus=[]\n","for dir in corpus_files:\n","  f = open(str(dir),'r', encoding = 'utf-8')\n","  content=f.readlines()\n","  tem_corpus = '\\n'.join(content)\n","  final_corpus.append(tem_corpus)"],"metadata":{"id":"tIcZc33ae5wU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(final_corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mw_68prMzODn","executionInfo":{"status":"ok","timestamp":1697339004091,"user_tz":-330,"elapsed":4,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"28dafaab-97e9-48aa-b24a-ea854ee39f6b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["182"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["## Sentence Count in 1st file of the whole corpus\n","sentences = nltk.sent_tokenize(final_corpus[0])\n","len(sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvYw_MjhuS-R","executionInfo":{"status":"ok","timestamp":1697339004735,"user_tz":-330,"elapsed":646,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"23f9a649-eeb8-4692-a87b-e17919b43cc4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["45241"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["Total_Sentence_count=0\n","for i in range(len(final_corpus)):\n","  Total_Sentence_count+=len(nltk.sent_tokenize(final_corpus[i]))\n"],"metadata":{"id":"0uMwPjim1mqT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Display the sentence count"],"metadata":{"id":"2Azf1GupWJZ6"}},{"cell_type":"code","source":["### Total sentences in whole corpus\n","Total_Sentence_count"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LaKDbPfyU5U1","executionInfo":{"status":"ok","timestamp":1697339153784,"user_tz":-330,"elapsed":21,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"03226d38-9c32-4b78-b79b-c230dd2ff3a8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7909406"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[],"metadata":{"id":"xlIlpS6sVbdW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5D0XasDRVbjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OLraCDo2VboY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Define function to get 4_Gram frequency distribution of the corpus\n","def get_freq_dist(corpus_text):\n","  tokenized_sentences = word_tokenize(corpus_text)\n","  tokenized_sentences= [token.lower() for token in tokenized_sentences]  ### Casefold all the words\n","  tokenized_sentences=[token for token in tokenized_sentences if token not in string.punctuation and token.isalpha()]  ### Remove Punctuations and Neumaric/Alpha Neumarics\n","  n = 4\n","  four_gram_model = list(ngrams([word for word in tokenized_sentences], n))\n","  fdist = FreqDist(four_gram_model)\n","  fdist_counter=Counter(fdist)\n","  return fdist_counter\n"],"metadata":{"id":"CKXrH7iktRrP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Divide whole Corpus of 182 files in 6 batches of size [30,30,30,30,30,32]"],"metadata":{"id":"qItMOSSnZWjY"}},{"cell_type":"markdown","source":["#### Get frequency distribution of each batches using get_freq_dist() function"],"metadata":{"id":"GgHaIk-1Zbhv"}},{"cell_type":"code","source":["\n","l=[]\n","for i in range(150,182):\n","  print(i)\n","  l.append(get_freq_dist(final_corpus[i]))\n"],"metadata":{"id":"VVW7K4gRIVbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fdist_counter_182=sum(l, Counter())"],"metadata":{"id":"IBVrbXZsVsV3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save the frequency distributions in google drive using pickle.dump()"],"metadata":{"id":"Ee6eYpQ6aQIL"}},{"cell_type":"code","source":["with open('/content/drive/MyDrive/NLP/Freq_Dist_182.pkl', 'wb') as fp:\n","    pickle.dump(fdist_counter_182, fp)"],"metadata":{"id":"xkqTd9d2Vsr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YboEZVHiVswm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Pickeled Frequency Distributions of all 6 batches\n","pkl_list=['/content/drive/MyDrive/NLP/Freq_Dist_30.pkl','/content/drive/MyDrive/NLP/Freq_Dist_60.pkl',\n","          '/content/drive/MyDrive/NLP/Freq_Dist_90.pkl','/content/drive/MyDrive/NLP/Freq_Dist_120.pkl',\n","          '/content/drive/MyDrive/NLP/Freq_Dist_150.pkl','/content/drive/MyDrive/NLP/Freq_Dist_182.pkl']"],"metadata":{"id":"hoWqD_Wlu7ej"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define a function to build a 4-gram language model."],"metadata":{"id":"8zb_8V8w1gSA"}},{"cell_type":"code","source":["### Define function to predict the next possible word\n","def new_word_pred(trigram):\n","  possible_next_words_counter=Counter()\n","  for i in pkl_list:\n","    # print(i)\n","    file = open(i,'rb')\n","    freq_dist = pickle.load(file)\n","    possible_next_words = {item:freq for item, freq in freq_dist.items() if item[:3] == trigram}\n","    possible_next_words =Counter(possible_next_words)\n","    possible_next_words_counter=possible_next_words_counter+possible_next_words\n","    del freq_dist\n","    del possible_next_words\n","  if not possible_next_words_counter:\n","        random_words = word_tokenize(final_corpus[random.choice(range(len(final_corpus)))])\n","        random_words= [token.lower() for token in random_words]  ### Casefold all the words\n","        random_words=[token for token in random_words if token not in string.punctuation and token.isalpha()]\n","        return random.choice(random_words)\n","  total_count = sum(possible_next_words_counter.values())\n","  # next_word_probabilities = [(item[3], freq / total_count) for item, freq in possibale_next_words]\n","  next_word_probabilities = {item[3]: (freq / total_count) for item, freq in possible_next_words_counter.items()}\n","  # print(len(possible_next_words_counter))\n","  return (sorted(next_word_probabilities.items(), key=lambda x: x[1])[-1][0])\n"],"metadata":{"id":"f_n4yY8-ItAU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Next word prediction - Test it to find the next word using the probability distribution.\n","â€¢ Take any word sequence (trigrams) and find the next word. Display the result of next\n","word prediction using 5 test sequences"],"metadata":{"id":"Vo1eD9_w1oDa"}},{"cell_type":"code","source":["# Test next word prediction\n","test_sequences = [(\"one\",\"of\",\"the\"),(\"end\",\"of\",\"the\"),(\"there\",\"is\",\"a\"),(\"out\",\"of\",\"the\"),(\"where\",\"are\",\"you\")]\n","for seq in test_sequences:\n","    next_word = new_word_pred(seq)\n","    print(f\"For trigram {seq}, predicted next word: {next_word}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RvgZ4hNwavq","executionInfo":{"status":"ok","timestamp":1697310290052,"user_tz":-330,"elapsed":716373,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"75945d56-6983-4a1a-a52a-b3b810faec64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["For trigram ('one', 'of', 'the'), predicted next word: most\n","For trigram ('end', 'of', 'the'), predicted next word: season\n","For trigram ('there', 'is', 'a'), predicted next word: small\n","For trigram ('out', 'of', 'the'), predicted next word: race\n","For trigram ('where', 'are', 'you'), predicted next word: going\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"QB6zdWONVbsT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define a function to generate sentences using previous next word prediction function"],"metadata":{"id":"BP2YjZba1yqp"}},{"cell_type":"code","source":["# Updated generate_sentence function\n","def generate_sentence(start_word, length=10):\n","    sentence = [start_word]\n","    for _ in range(length - 1):\n","        trigram = tuple(sentence[-3:])\n","        next_word = new_word_pred(trigram)\n","        if next_word:\n","            sentence.append(next_word)\n","        else:\n","            break\n","    # Convert the elements to strings before joining\n","    return \" \".join(str(word) for word in sentence)\n"],"metadata":{"id":"hdBbsd2xmdFR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9_jEWQLWBhfF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sentence generation - Using a random start word to construct a sentence of 10 words.\n","Generate 5 sentences and publish the result. Avoid using the, in, a or an as the starting\n","word"],"metadata":{"id":"vBEx_YEb2INV"}},{"cell_type":"code","source":["# Generate 5 sentences\n","for _ in range(5):\n","    random_words = word_tokenize(final_corpus[random.choice(range(len(final_corpus)))])\n","    random_words= [token.lower() for token in random_words]  ### Casefold all the words\n","    random_words=[token for token in random_words if token not in string.punctuation and token.isalpha()]\n","    start_word = random.choice([word for word in random_words if word not in [\"the\", \"in\", \"a\", \"an\"]])\n","    sentence = generate_sentence(start_word)\n","    print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697317957929,"user_tz":-330,"elapsed":3208814,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"81cc57e6-1ed8-4c9d-a21f-3316dd3f40d7","id":"shBLCut_mdFa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["church the osteopathic an historical his a a loader as\n","originally blind is nick in a root advances is with\n","kabaddi identified powell urchins initially played topics was population stating\n","tinderbox flowering fortified as the for the first time in\n","india olympics about event in the united states and canada\n"]}]},{"cell_type":"code","source":["# Generate 5 sentences\n","for _ in range(5):\n","    random_words = word_tokenize(final_corpus[random.choice(range(len(final_corpus)))])\n","    random_words= [token.lower() for token in random_words]  ### Casefold all the words\n","    random_words=[token for token in random_words if token not in string.punctuation and token.isalpha()]\n","    start_word = random.choice([word for word in random_words if word not in [\"the\", \"in\", \"a\", \"an\"]])\n","    sentence = generate_sentence(start_word)\n","    print(sentence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_i9KxjprVbv1","executionInfo":{"status":"ok","timestamp":1697343801209,"user_tz":-330,"elapsed":4606444,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"3d4339b7-9579-42c2-b61f-dd3370f143cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["with in panned cloudy bethel played signing american a hemel\n","run in ontario on the shores of the lake is\n","example in it was described by the same name by\n","love an border lead is structure elliott music but switching\n","minister tribes the is and the community of the basque\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ymc62E52maTo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 2 (15 Marks)\n","### (1) Use NLTK to tag all sentences of the corpus using Parts of Speech (POS)\n","### (2) Recompose the entire corpus using POS instead of original words\n","â€¢ Recompose it sentence by sentence and save them in a file.\n","â€¢ Find the frequency of each structure\n","â€¢ Display the total number of unique structures\n","### (3) List the most common (5 structures) POS structures/sentence/Grammatical structures.\n","â€¢ This should be done for each sentence in the corpus"],"metadata":{"id":"w2d1lB-k2QnO"}},{"cell_type":"code","source":[],"metadata":{"id":"OOXrrcrW213L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use NLTK to tag all sentences of the corpus using Parts of Speech (POS) and recompose the entire corpus using POS instead of original words and save it in google drive for later use"],"metadata":{"id":"KUiO4ldS2oFR"}},{"cell_type":"code","source":["structure_counter=Counter()\n","unique_structure_list=[]\n","for i in range(len(final_corpus)):\n","  # Your input corpus\n","  corpus = final_corpus[i]\n","  # (1) Tag all sentences of the corpus using Parts of Speech (POS)\n","  sentences = sent_tokenize(corpus)\n","  def clean_sentence(sentence):\n","      words = word_tokenize(sentence)\n","      cleaned_words = [word for word in words if word.isalpha()]  # Remove numeric/alpha-numeric\n","      cleaned_sentence = ' '.join(cleaned_words)\n","      return cleaned_sentence\n","\n","  tagged_sentences = [pos_tag(word_tokenize(clean_sentence(sentence))) for sentence in sentences]\n","\n","  # (2) Recompose the entire corpus using POS instead of original words\n","  # Recompose it sentence by sentence and save them in a file\n","  with open(r'/content/drive/MyDrive/NLP/recomposed_corpus/'+str(i)+'_POS.txt', 'w') as output_file:\n","      for tagged_sentence in tagged_sentences:\n","          recomposed_sentence = ' '.join([tag for word, tag in tagged_sentence])\n","          output_file.write(recomposed_sentence + '\\n')\n","\n","  # (3) Find the frequency of each structure\n","  # Display the total number of unique structures\n","  structures = [' '.join([tag for word, tag in tagged_sentence]) for tagged_sentence in tagged_sentences]\n","\n","  print(\"Total number of unique structures in file \"+str(i)+\" is : \"+str(len(Counter(structures))))\n","  unique_structure_list.append(len(Counter(structures)))\n","\n","  structure_counter =structure_counter+ Counter(structures)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fddN1mzIoZha","executionInfo":{"status":"ok","timestamp":1697277298167,"user_tz":-330,"elapsed":3805650,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"1ae30535-f9e9-4c73-cfa0-4ae212c74d7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of unique structures in file 0 is : 39875\n","Total number of unique structures in file 1 is : 36073\n","Total number of unique structures in file 2 is : 37442\n","Total number of unique structures in file 3 is : 39736\n","Total number of unique structures in file 4 is : 37787\n","Total number of unique structures in file 5 is : 38129\n","Total number of unique structures in file 6 is : 24903\n","Total number of unique structures in file 7 is : 38137\n","Total number of unique structures in file 8 is : 35951\n","Total number of unique structures in file 9 is : 35932\n","Total number of unique structures in file 10 is : 39055\n","Total number of unique structures in file 11 is : 39063\n","Total number of unique structures in file 12 is : 37213\n","Total number of unique structures in file 13 is : 37049\n","Total number of unique structures in file 14 is : 41146\n","Total number of unique structures in file 15 is : 36920\n","Total number of unique structures in file 16 is : 35216\n","Total number of unique structures in file 17 is : 37055\n","Total number of unique structures in file 18 is : 36647\n","Total number of unique structures in file 19 is : 35950\n","Total number of unique structures in file 20 is : 39563\n","Total number of unique structures in file 21 is : 37982\n","Total number of unique structures in file 22 is : 37599\n","Total number of unique structures in file 23 is : 36129\n","Total number of unique structures in file 24 is : 36920\n","Total number of unique structures in file 25 is : 33939\n","Total number of unique structures in file 26 is : 37088\n","Total number of unique structures in file 27 is : 37144\n","Total number of unique structures in file 28 is : 38106\n","Total number of unique structures in file 29 is : 37366\n","Total number of unique structures in file 30 is : 35800\n","Total number of unique structures in file 31 is : 37680\n","Total number of unique structures in file 32 is : 34167\n","Total number of unique structures in file 33 is : 40094\n","Total number of unique structures in file 34 is : 42057\n","Total number of unique structures in file 35 is : 39227\n","Total number of unique structures in file 36 is : 36642\n","Total number of unique structures in file 37 is : 39416\n","Total number of unique structures in file 38 is : 37019\n","Total number of unique structures in file 39 is : 38307\n","Total number of unique structures in file 40 is : 36193\n","Total number of unique structures in file 41 is : 36474\n","Total number of unique structures in file 42 is : 36851\n","Total number of unique structures in file 43 is : 42522\n","Total number of unique structures in file 44 is : 37248\n","Total number of unique structures in file 45 is : 42023\n","Total number of unique structures in file 46 is : 36382\n","Total number of unique structures in file 47 is : 35435\n","Total number of unique structures in file 48 is : 34823\n","Total number of unique structures in file 49 is : 38184\n","Total number of unique structures in file 50 is : 39470\n","Total number of unique structures in file 51 is : 39001\n","Total number of unique structures in file 52 is : 37505\n","Total number of unique structures in file 53 is : 35678\n","Total number of unique structures in file 54 is : 35520\n","Total number of unique structures in file 55 is : 37364\n","Total number of unique structures in file 56 is : 37704\n","Total number of unique structures in file 57 is : 32417\n","Total number of unique structures in file 58 is : 39928\n","Total number of unique structures in file 59 is : 39992\n","Total number of unique structures in file 60 is : 36682\n","Total number of unique structures in file 61 is : 40992\n","Total number of unique structures in file 62 is : 34604\n","Total number of unique structures in file 63 is : 39170\n","Total number of unique structures in file 64 is : 37890\n","Total number of unique structures in file 65 is : 38166\n","Total number of unique structures in file 66 is : 37595\n","Total number of unique structures in file 67 is : 43111\n","Total number of unique structures in file 68 is : 38060\n","Total number of unique structures in file 69 is : 36824\n","Total number of unique structures in file 70 is : 38524\n","Total number of unique structures in file 71 is : 42697\n","Total number of unique structures in file 72 is : 41239\n","Total number of unique structures in file 73 is : 35696\n","Total number of unique structures in file 74 is : 34051\n","Total number of unique structures in file 75 is : 39929\n","Total number of unique structures in file 76 is : 37211\n","Total number of unique structures in file 77 is : 41477\n","Total number of unique structures in file 78 is : 40268\n","Total number of unique structures in file 79 is : 38624\n","Total number of unique structures in file 80 is : 35453\n","Total number of unique structures in file 81 is : 38432\n","Total number of unique structures in file 82 is : 32791\n","Total number of unique structures in file 83 is : 36745\n","Total number of unique structures in file 84 is : 32196\n","Total number of unique structures in file 85 is : 37735\n","Total number of unique structures in file 86 is : 34407\n","Total number of unique structures in file 87 is : 38828\n","Total number of unique structures in file 88 is : 33980\n","Total number of unique structures in file 89 is : 34771\n","Total number of unique structures in file 90 is : 36839\n","Total number of unique structures in file 91 is : 35461\n","Total number of unique structures in file 92 is : 39013\n","Total number of unique structures in file 93 is : 37516\n","Total number of unique structures in file 94 is : 36162\n","Total number of unique structures in file 95 is : 37572\n","Total number of unique structures in file 96 is : 41222\n","Total number of unique structures in file 97 is : 37280\n","Total number of unique structures in file 98 is : 37189\n","Total number of unique structures in file 99 is : 39087\n","Total number of unique structures in file 100 is : 36305\n","Total number of unique structures in file 101 is : 41877\n","Total number of unique structures in file 102 is : 34604\n","Total number of unique structures in file 103 is : 36884\n","Total number of unique structures in file 104 is : 38101\n","Total number of unique structures in file 105 is : 36261\n","Total number of unique structures in file 106 is : 36457\n","Total number of unique structures in file 107 is : 41424\n","Total number of unique structures in file 108 is : 35888\n","Total number of unique structures in file 109 is : 40427\n","Total number of unique structures in file 110 is : 39463\n","Total number of unique structures in file 111 is : 37106\n","Total number of unique structures in file 112 is : 36828\n","Total number of unique structures in file 113 is : 36200\n","Total number of unique structures in file 114 is : 37112\n","Total number of unique structures in file 115 is : 34102\n","Total number of unique structures in file 116 is : 37328\n","Total number of unique structures in file 117 is : 36299\n","Total number of unique structures in file 118 is : 36191\n","Total number of unique structures in file 119 is : 38204\n","Total number of unique structures in file 120 is : 37328\n","Total number of unique structures in file 121 is : 37029\n","Total number of unique structures in file 122 is : 23807\n","Total number of unique structures in file 123 is : 36125\n","Total number of unique structures in file 124 is : 39411\n","Total number of unique structures in file 125 is : 35421\n","Total number of unique structures in file 126 is : 32812\n","Total number of unique structures in file 127 is : 34529\n","Total number of unique structures in file 128 is : 39016\n","Total number of unique structures in file 129 is : 38454\n","Total number of unique structures in file 130 is : 39913\n","Total number of unique structures in file 131 is : 36751\n","Total number of unique structures in file 132 is : 13260\n","Total number of unique structures in file 133 is : 36138\n","Total number of unique structures in file 134 is : 35880\n","Total number of unique structures in file 135 is : 36225\n","Total number of unique structures in file 136 is : 38161\n","Total number of unique structures in file 137 is : 38163\n","Total number of unique structures in file 138 is : 39752\n","Total number of unique structures in file 139 is : 36260\n","Total number of unique structures in file 140 is : 36996\n","Total number of unique structures in file 141 is : 40196\n","Total number of unique structures in file 142 is : 37755\n","Total number of unique structures in file 143 is : 39822\n","Total number of unique structures in file 144 is : 37262\n","Total number of unique structures in file 145 is : 36151\n","Total number of unique structures in file 146 is : 37636\n","Total number of unique structures in file 147 is : 40542\n","Total number of unique structures in file 148 is : 36713\n","Total number of unique structures in file 149 is : 38029\n","Total number of unique structures in file 150 is : 38124\n","Total number of unique structures in file 151 is : 39382\n","Total number of unique structures in file 152 is : 37226\n","Total number of unique structures in file 153 is : 37945\n","Total number of unique structures in file 154 is : 42543\n","Total number of unique structures in file 155 is : 40206\n","Total number of unique structures in file 156 is : 37939\n","Total number of unique structures in file 157 is : 38741\n","Total number of unique structures in file 158 is : 27321\n","Total number of unique structures in file 159 is : 37302\n","Total number of unique structures in file 160 is : 36785\n","Total number of unique structures in file 161 is : 36901\n","Total number of unique structures in file 162 is : 37374\n","Total number of unique structures in file 163 is : 37080\n","Total number of unique structures in file 164 is : 40679\n","Total number of unique structures in file 165 is : 38948\n","Total number of unique structures in file 166 is : 37765\n","Total number of unique structures in file 167 is : 36238\n","Total number of unique structures in file 168 is : 36794\n","Total number of unique structures in file 169 is : 38440\n","Total number of unique structures in file 170 is : 40659\n","Total number of unique structures in file 171 is : 36836\n","Total number of unique structures in file 172 is : 41621\n","Total number of unique structures in file 173 is : 36988\n","Total number of unique structures in file 174 is : 40568\n","Total number of unique structures in file 175 is : 39675\n","Total number of unique structures in file 176 is : 37779\n","Total number of unique structures in file 177 is : 41058\n","Total number of unique structures in file 178 is : 37289\n","Total number of unique structures in file 179 is : 36117\n","Total number of unique structures in file 180 is : 39189\n","Total number of unique structures in file 181 is : 38115\n"]}]},{"cell_type":"markdown","source":["### Total number of unique POS structures in whole corpus"],"metadata":{"id":"3jT4G1LZ2-AH"}},{"cell_type":"code","source":["len(structure_counter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_R3f9PW8ZZ1V","executionInfo":{"status":"ok","timestamp":1697277336514,"user_tz":-330,"elapsed":386,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"d1f9cf17-bf1e-4762-b1fb-3c66ef67c422"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6162756"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["common_structure_counter=sorted(structure_counter.items(), key=lambda x: x[1])[-5:]"],"metadata":{"id":"Ga_w6WbTXPrQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" ## List the most common (5 structures) POS structures/sentence/Grammatical structures"],"metadata":{"id":"7AQSru1G3R6v"}},{"cell_type":"code","source":["common_structure_counter"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o0NNoezEuwqf","executionInfo":{"status":"ok","timestamp":1697277346441,"user_tz":-330,"elapsed":454,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"a6b4b0fe-482f-43f3-f0a1-561887e5c8f3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('NNP VBZ DT NN IN NNS IN DT NN NNP', 7726),\n"," ('NNP NN VBZ DT NNS IN NN IN DT NNP NN', 9363),\n"," ('NNP NN VBZ DT NN IN DT NNP NN', 9834),\n"," ('NNP NN VBZ DT NN IN DT NN NNP', 16792),\n"," ('NNP NN VBZ DT NNS IN NN IN DT NN NNP', 26438)]"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["common_structure_keys=[]\n","\n","for i in common_structure_counter:\n","  common_structure_keys.append(i[0])"],"metadata":{"id":"4dXqdM9tu5KB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["common_structure_keys"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LOoqxQrTwi5A","executionInfo":{"status":"ok","timestamp":1697281740412,"user_tz":-330,"elapsed":1445,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"726b38e8-71fc-49f1-d162-a30f162aafb1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['NNP VBZ DT NN IN NNS IN DT NN NNP',\n"," 'NNP NN VBZ DT NNS IN NN IN DT NNP NN',\n"," 'NNP NN VBZ DT NN IN DT NNP NN',\n"," 'NNP NN VBZ DT NN IN DT NN NNP',\n"," 'NNP NN VBZ DT NNS IN NN IN DT NN NNP']"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":[],"metadata":{"id":"Au0Emfqz3Yze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save the structer frequencies of full corpus and most common structures for later use"],"metadata":{"id":"M55GDLvI3vfq"}},{"cell_type":"code","source":["import pickle\n","with open('/content/drive/MyDrive/NLP/structure_counter.pkl', 'wb') as fp:\n","    pickle.dump(structure_counter, fp)"],"metadata":{"id":"Ajsf5QJh4JDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","with open('/content/drive/MyDrive/NLP/common_structure_counter.pkl', 'wb') as fp:\n","    pickle.dump(common_structure_counter, fp)"],"metadata":{"id":"PDe6s7TA4WOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1UBxbZrh32e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DJ7vHcQ832kO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3_IgDZ3632qA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M2tHr8el32uI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bqQo_Q7232w9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gjSXlZNA32zz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Part 3 (20 Marks)\n","#(1) Generate a sentence each using the five common structures\n","â€¢ For the same starting word, generate 5 sentences using the five structures. Avoid using\n","the, in, a or an as the starting word\n","â€¢ Total number of sentences to display - 25 sentences"],"metadata":{"id":"4ZLpJQpP39HP"}},{"cell_type":"code","source":[],"metadata":{"id":"MqyAzvNs3229"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","# Define the five common POS structures\n","common_structures = common_structure_keys\n","\n","# Sample words to start the sentences\n","starting_words = [\"The\", \"Quick\", \"Brown\", \"Fox\", \"Jumped\"]\n","\n","# Generate 5 sentences for each common structure\n","sentences = []\n","for structure in common_structures:\n","    for word in starting_words:\n","        if word not in [\"The\", \"In\", \"A\", \"An\"]:\n","            sentence = f\"{word} {structure}\"\n","            sentences.append(sentence)\n","\n","# Shuffle the generated sentences\n","random.shuffle(sentences)\n","\n","# Display a total of 25 sentences\n","for i, sentence in enumerate(sentences[:25], 1):\n","    print(f\"Sentence {i}: {sentence}\")\n"],"metadata":{"id":"4ux0gGEkXQCl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697277355804,"user_tz":-330,"elapsed":355,"user":{"displayName":"Akash Das","userId":"14393997291477237156"}},"outputId":"6e50e53f-4ce1-46d8-84cb-4e4664409b0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence 1: Brown NNP NN VBZ DT NNS IN NN IN DT NNP NN\n","Sentence 2: Quick NNP NN VBZ DT NN IN DT NNP NN\n","Sentence 3: Fox NNP VBZ DT NN IN NNS IN DT NN NNP\n","Sentence 4: Fox NNP NN VBZ DT NNS IN NN IN DT NNP NN\n","Sentence 5: Fox NNP NN VBZ DT NN IN DT NN NNP\n","Sentence 6: Jumped NNP VBZ DT NN IN NNS IN DT NN NNP\n","Sentence 7: Jumped NNP NN VBZ DT NNS IN NN IN DT NN NNP\n","Sentence 8: Brown NNP NN VBZ DT NN IN DT NN NNP\n","Sentence 9: Fox NNP NN VBZ DT NNS IN NN IN DT NN NNP\n","Sentence 10: Quick NNP NN VBZ DT NN IN DT NN NNP\n","Sentence 11: Brown NNP NN VBZ DT NN IN DT NNP NN\n","Sentence 12: Quick NNP VBZ DT NN IN NNS IN DT NN NNP\n","Sentence 13: Jumped NNP NN VBZ DT NN IN DT NNP NN\n","Sentence 14: Jumped NNP NN VBZ DT NN IN DT NN NNP\n","Sentence 15: Quick NNP NN VBZ DT NNS IN NN IN DT NNP NN\n","Sentence 16: Fox NNP NN VBZ DT NN IN DT NNP NN\n","Sentence 17: Quick NNP NN VBZ DT NNS IN NN IN DT NN NNP\n","Sentence 18: Brown NNP NN VBZ DT NNS IN NN IN DT NN NNP\n","Sentence 19: Jumped NNP NN VBZ DT NNS IN NN IN DT NNP NN\n","Sentence 20: Brown NNP VBZ DT NN IN NNS IN DT NN NNP\n"]}]},{"cell_type":"code","source":["nltk.download('brown')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JO5dflwlpmQv","executionInfo":{"status":"ok","timestamp":1698167157155,"user_tz":-330,"elapsed":1331,"user":{"displayName":"Shreyan Chakraborty","userId":"08730152304516727362"}},"outputId":"c8080148-83d6-4af6-af6c-ac4fbfc1d858"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["nltk.download('universal_tagset')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8tWB7mUqHs1","executionInfo":{"status":"ok","timestamp":1698167293373,"user_tz":-330,"elapsed":604,"user":{"displayName":"Shreyan Chakraborty","userId":"08730152304516727362"}},"outputId":"5438b9c0-9bbe-4738-f7fe-d64be0e9c4fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["from nltk.corpus import brown"],"metadata":{"id":"srzKSYlQoyCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize"],"metadata":{"id":"2lSzuBYQpa-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List of starting words (customize as needed)\n","starting_words = [\"apple\", \"book\", \"car\", \"dog\", \"house\"]\n","\n","# Five common grammatical structures\n","structures = common_structure_keys\n","# Words to avoid as starting words\n","avoid_words = {\"the\", \"in\", \"a\", \"an\"}\n","\n"],"metadata":{"id":"8Rkn7iJ7cC5i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to generate a sentence for a given structure and starting word\n","def generate_sentence(structure, starting_word):\n","    sentence = structure.replace(\"Noun\", starting_word)\n","    return sentence"],"metadata":{"id":"kM9cGaaacM9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate 5 sentences for each structure with random starting words\n","sentences = []\n","for structure in structures:\n","    for _ in range(5):\n","        # Choose a random starting word from the list\n","        starting_word = random.choice(starting_words)\n","        # Ensure the starting word is not in the list of avoided words\n","        while starting_word.lower() in avoid_words:\n","            starting_word = random.choice(starting_words)\n","        sentence = generate_sentence(structure, starting_word)\n","        sentence = \" \".join([starting_word,sentence])\n","        sentences.append(sentence)"],"metadata":{"id":"VLvodPk3cQpb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Only with pos_tags"],"metadata":{"id":"3TxnuvpPyuox"}},{"cell_type":"code","source":["# Display the generated sentences\n","for i, sentence in enumerate(sentences, start=1):\n","    print(f\"Sentence {i}: {sentence}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1w4utdV0cTM1","executionInfo":{"status":"ok","timestamp":1698168425235,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shreyan Chakraborty","userId":"08730152304516727362"}},"outputId":"3c3ac8b6-79c2-4e8d-8efc-da72093f31a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence 1: dog NN NN VBZ DT NNS IN NN IN DT NN NN .\n","Sentence 2: house NN NN VBZ DT NNS IN NN IN DT NN NN .\n","Sentence 3: book NN NN VBZ DT NNS IN NN IN DT NN NN .\n","Sentence 4: house NN NN VBZ DT NNS IN NN IN DT NN NN .\n","Sentence 5: apple NN NN VBZ DT NNS IN NN IN DT NN NN .\n","Sentence 6: book NN .\n","Sentence 7: apple NN .\n","Sentence 8: dog NN .\n","Sentence 9: house NN .\n","Sentence 10: car NN .\n","Sentence 11: apple NN NN VBZ DT NN IN DT NN NN .\n","Sentence 12: dog NN NN VBZ DT NN IN DT NN NN .\n","Sentence 13: house NN NN VBZ DT NN IN DT NN NN .\n","Sentence 14: apple NN NN VBZ DT NN IN DT NN NN .\n","Sentence 15: car NN NN VBZ DT NN IN DT NN NN .\n","Sentence 16: book NN NN VBZ DT NN IN DT JJ NN .\n","Sentence 17: apple NN NN VBZ DT NN IN DT JJ NN .\n","Sentence 18: apple NN NN VBZ DT NN IN DT JJ NN .\n","Sentence 19: book NN NN VBZ DT NN IN DT JJ NN .\n","Sentence 20: book NN NN VBZ DT NN IN DT JJ NN .\n","Sentence 21: book NN NN VBZ DT NNS IN NN IN DT JJ NN .\n","Sentence 22: book NN NN VBZ DT NNS IN NN IN DT JJ NN .\n","Sentence 23: house NN NN VBZ DT NNS IN NN IN DT JJ NN .\n","Sentence 24: car NN NN VBZ DT NNS IN NN IN DT JJ NN .\n","Sentence 25: book NN NN VBZ DT NNS IN NN IN DT JJ NN .\n"]}]},{"cell_type":"code","source":["from nltk.corpus import treebank\n","\n","nltk.download('treebank')\n","nltk.download('maxent_treebank_pos_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VjhC7rHuOX7","executionInfo":{"status":"ok","timestamp":1698168374039,"user_tz":-330,"elapsed":1348,"user":{"displayName":"Shreyan Chakraborty","userId":"08730152304516727362"}},"outputId":"9e98bf02-2cce-423a-f304-d4fbd1acb5f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n","[nltk_data] Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","source":["tag_sentences = treebank.tagged_sents()\n","\n","# Build a mapping from POS tags to words\n","pos_to_words = defaultdict(list)\n","for sentence in tag_sentences:\n","    for word, pos in sentence:\n","        pos_to_words[pos].append(word)"],"metadata":{"id":"WZ-WFVE_sQZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to convert POS tags to words\n","def pos_to_words_fn(pos_sentence):\n","    words = []\n","    for pos_tag in pos_sentence:\n","        word = random.choice(brown.tagged_words(tagset='universal')).word\n","        words.append(word)\n","    return \" \".join(words)"],"metadata":{"id":"fEY4oul8pKLV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# All words in the corpus\n","all_words = [word for sentence in treebank.sents() for word in sentence]\n","\n","# Define a POS tagger\n","pos_tagger = nltk.data.load(\"taggers/maxent_treebank_pos_tagger/english.pickle\")\n"],"metadata":{"id":"Fqspg3GXuo-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Your 4-gram language model (replace this with your actual ngram_model)\n","ngram_model = four_gram_model\n","\n","# Define the structures for sentence generation\n","structures = common_structure_keys"],"metadata":{"id":"7Kit42pYiD2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_original_sentence(structure, starting_word):\n","    words = [starting_word]\n","    current_pos = pos_tagger.tag([starting_word])[0][1]  # Get the POS tag of the starting word\n","    structure_pos = structure.split()\n","\n","    for pos in structure_pos[1:]:  # Skip the first POS as it is for the starting word\n","        if pos in pos_to_words:\n","            word = random.choice(pos_to_words[pos])\n","            words.append(word)\n","        else:\n","            words.append(pos)\n","    return ' '.join(words).capitalize()\n","\n"],"metadata":{"id":"tRs4D1dXu6NW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Replaced with actual words"],"metadata":{"id":"75g3JSZfyytQ"}},{"cell_type":"code","source":["# Generate and display sentences\n","for _ in range(25):\n","    starting_word = random.choice(starting_words)\n","    structure = random.choice(structures)\n","    sentence = generate_original_sentence(structure, starting_word)\n","    print(f\"Sentence {_+1}: {sentence}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zc4FtjEbvAAe","executionInfo":{"status":"ok","timestamp":1698169438704,"user_tz":-330,"elapsed":1461,"user":{"displayName":"Shreyan Chakraborty","userId":"08730152304516727362"}},"outputId":"17204e0a-f927-4e96-e6b5-799f1e133c67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence 1: Car report is an temperature of the able bottle .\n","Sentence 2: House list has this sums in globe by a average health .\n","Sentence 3: House .\n","Sentence 4: Apple consideration says a sales for % into an separate rate .\n","Sentence 5: Book charge expects the associate on a facility charge .\n","Sentence 6: Book .\n","Sentence 7: Car attempt is a appropriations in addition in a primary factory .\n","Sentence 8: Apple name has a breaker at the pursuant bill .\n","Sentence 9: Car volatility gives the struggle into the initial compromise .\n","Sentence 10: Car country 's the sales for limit of the beautiful home .\n","Sentence 11: Apple employer is a terms in speculation as a chauffeur % .\n","Sentence 12: House woman wants the amendment of the christian share .\n","Sentence 13: Apple school says the week by the local return .\n","Sentence 14: Book market eliminates a products of fit as the homelessness news .\n","Sentence 15: Dog .\n","Sentence 16: Dog year says a institutions in sympathy of the angry evidence .\n","Sentence 17: House district has the government for the administration public .\n","Sentence 18: Dog stability is the honors after price of the financial yen .\n","Sentence 19: Car .\n","Sentence 20: Car shape says a underwriter by these year survival .\n","Sentence 21: Dog irony is a rape by this service company .\n","Sentence 22: Apple stock flies a investors for right for a wrong cost .\n","Sentence 23: Book .\n","Sentence 24: Dog .\n","Sentence 25: Apple .\n"]}]},{"cell_type":"markdown","source":["________________________________________________________________________________________________________________________________________________________________"],"metadata":{"id":"5vBM_rV1y4KJ"}}]}